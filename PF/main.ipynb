{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent recommender networks\n",
    "## Trabalho final da disciplina de Introdução à Machine Learning - UFRJ 2022.2\n",
    "\n",
    "Baseado no artigo [Wu, Chao-Yuan, et al. 'Recurrent recommender networks.' 2017.](https://dl.acm.org/doi/epdf/10.1145/3018661.3018689)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.nn as nn\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        1      1193       5  978300760\n",
       "1        1       661       3  978302109\n",
       "2        1       914       3  978301968\n",
       "3        1      3408       4  978300275\n",
       "4        1      2355       5  978824291"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_titles = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "\n",
    "df = pd.read_table(\n",
    "    'ratings.dat',\n",
    "    sep = '::',\n",
    "    header = None,\n",
    "    names = ratings_titles,\n",
    "    engine = 'python'\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset possui 1000209 instâncias.\n"
     ]
    }
   ],
   "source": [
    "print(f'O dataset possui {df.shape[0]} instâncias.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dessas instâncias, temos 6040 usuários, e 3706 filmes.\n"
     ]
    }
   ],
   "source": [
    "num_users = df['user_id'].nunique()\n",
    "num_movies = df['movie_id'].nunique()\n",
    "\n",
    "print(f'Dessas instâncias, temos {num_users} usuários, e {num_movies} filmes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entretanto, os identificadores dos filmes não são sequencias, como os dos usuários, e por essa razão teve que ser alterado para o maior identificador existente no dataset.\n",
      "Do contrário, a célula de treinamento dá erro em relação aos índices dos arrays.\n",
      "Número de filmes considerados é, então, igual à 3952.\n"
     ]
    }
   ],
   "source": [
    "num_movies = max(df['movie_id'])\n",
    "\n",
    "print(\n",
    "    'Entretanto, os identificadores dos filmes não são sequencias, como os dos usuários, '\n",
    "    'e por essa razão teve que ser alterado para o maior identificador existente no dataset.'\n",
    "    '\\nDo contrário, a célula de treinamento dá erro em relação aos índices dos arrays.'\n",
    "    f'\\nNúmero de filmes considerados é, então, igual à {num_movies}.'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o modelo LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAM = {\n",
    "    'batch_size': 50,\n",
    "    'hidden_size': 128,\n",
    "    'out_size': 64,\n",
    "    'n_step': 1,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 10\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()\n",
    "\n",
    "user_id = tf.placeholder(tf.int32, shape = [None, 1], name = 'user_id')\n",
    "movie_id = tf.placeholder(tf.int32, shape = [None, 1], name = 'movie_id')\n",
    "rating = tf.placeholder(tf.float32, shape = [None, 1], name = 'rating')\n",
    "dropout = tf.placeholder(tf.float32, name = 'dropout')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camadas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User embedding layer & lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('user_id_embedding', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    embedding_user = tf.get_variable(\n",
    "        name = 'embedding_users',\n",
    "        shape = [num_users, HYPERPARAM['hidden_size']],\n",
    "        initializer = tf.glorot_uniform_initializer()\n",
    "    )\n",
    "\n",
    "    user_id_layer = nn.embedding_lookup(embedding_user, user_id)\n",
    "    user_id_layer = nn.relu(user_id_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie embedding layer & lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('movie_id_embedding', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    embedding_movie = tf.get_variable(\n",
    "        name = 'embedding_movie',\n",
    "        shape = [num_movies, HYPERPARAM['hidden_size']],\n",
    "        initializer = tf.glorot_uniform_initializer()\n",
    "    )\n",
    "\n",
    "    movie_id_layer = nn.embedding_lookup(embedding_movie, movie_id)\n",
    "    movie_id_layer = nn.relu(movie_id_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10740\\1884813009.py:5: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('user_rnn_cell', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    user_cell = tf.keras.layers.LSTMCell(HYPERPARAM['hidden_size'])\n",
    "    user_input = tf.transpose(user_id_layer, [1, 0, 2])\n",
    "    user_outputs, user_states = tf.nn.dynamic_rnn(user_cell, user_input, dtype = tf.float32)\n",
    "    user_output = user_outputs[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('movie_rnn_cell', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    movie_cell =  tf.keras.layers.LSTMCell(HYPERPARAM['hidden_size'])\n",
    "    movie_input  = tf.transpose(movie_id_layer, [1, 0, 2])\n",
    "    movie_outputs, movie_states = tf.nn.dynamic_rnn(movie_cell, movie_input, dtype = tf.float32)\n",
    "    movie_output = movie_outputs[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('pred_layer', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    user_vector = tf.layers.dense(user_output, HYPERPARAM['out_size'], activation = None)\n",
    "    movie_vector = tf.layers.dense(movie_output, HYPERPARAM['out_size'], activation = None)\n",
    "\n",
    "    pred = tf.reduce_sum(tf.multiply(user_vector, movie_vector), axis = 1, keepdims = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss e Otimizador\n",
    "- Loss: erro quadrático médio\n",
    "- Otimizador: ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.losses.mean_squared_error(rating, pred))\n",
    "optimizer = tf.train.AdamOptimizer(HYPERPARAM['learning_rate']).minimize(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teremos 20005 batches avaliando 50 amostras cada.\n",
      "20005 (Total de batches) x 50 (Amostras por batches) = 1000250 =~ 1000209 (Tamanho do dataset).\n"
     ]
    }
   ],
   "source": [
    "train = df.values\n",
    "length  = len(train)\n",
    "batches = int(length/ HYPERPARAM['batch_size']) + 1\n",
    "\n",
    "print(\n",
    "    f\"Teremos {batches} batches avaliando {HYPERPARAM['batch_size']} amostras cada.\\n\"\n",
    "    f\"{batches} (Total de batches) x {HYPERPARAM['batch_size']} (Amostras por batches) = \"\n",
    "    f\"{batches * HYPERPARAM['batch_size']} =~ {df.shape[0]} (Tamanho do dataset).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #20000 de 20005: loss = 0.9451911449432373\n",
      "LSTM levou um total de 789.8618821 segundos = 13.164364701666667 minutos.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "for batch in range(batches):\n",
    "    \n",
    "    # Definindo limite superior e inferior dos índices para cada batch\n",
    "    min_index = batch * HYPERPARAM['batch_size']\n",
    "    max_index = min(length, (batch + 1) * HYPERPARAM['batch_size'])\n",
    "    train_batch =  train[min_index : max_index]\n",
    "\n",
    "    # user, movie, rating\n",
    "    inputs  = np.array([\n",
    "        (batch[0] - 1, batch[1] - 1, float(batch[2])) for batch in train_batch\n",
    "    ])\n",
    "\n",
    "    # Não é possível passar os placeholders, então usamos feed_dict  \n",
    "    feed_dict = {\n",
    "        user_id: np.expand_dims(inputs[:, 0], 1),\n",
    "        movie_id: np.expand_dims(inputs[:, 1], 1),\n",
    "        rating: np.expand_dims(inputs[:, 2], 1),\n",
    "        dropout: 1.0\n",
    "    }\n",
    "\n",
    "    # Treinamento\n",
    "    _, batch_loss = session.run([optimizer, loss], feed_dict = feed_dict)\n",
    "\n",
    "    # Salvando valores de loss para futuro plot\n",
    "    train_loss.append(batch_loss)\n",
    "\n",
    "    # Criando DataFrame com as previsões\n",
    "    prediction = session.run(pred, feed_dict = feed_dict)\n",
    "    df_prediction = {\n",
    "        'Actual': inputs[:, 2],\n",
    "        'Predicted': prediction.reshape(-1)\n",
    "    }\n",
    "    df_prediction = pd.DataFrame.from_dict(df_prediction, orient = 'index').T\n",
    "\n",
    "    # Print para debug que evita logs longos\n",
    "    if HYPERPARAM['verbose'] and (batch % HYPERPARAM['verbose'] == 0):\n",
    "        sys.stdout.write(\n",
    "            '\\rBatch #{} de {}: loss = {}'.format(\n",
    "                batch, batches, np.sqrt(np.mean(train_loss[-HYPERPARAM['batch_size']:]))\n",
    "            )\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "total_time = time.perf_counter() - start_time\n",
    "print(f'\\nLSTM levou um total de {total_time} segundos = {total_time/ 60} minutos.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
